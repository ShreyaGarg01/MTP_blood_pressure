{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import ElasticNet, RidgeClassifier, Lasso, PassiveAggressiveClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from hyperopt import hp\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the uploaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"Data\\\\BP_features.csv\")\n",
    "labels = pd.read_csv(\"Data\\\\final_labels.csv\")\n",
    "labels = labels.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_normal = 0\n",
    "for i in labels:\n",
    "    count_normal += i\n",
    "count_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the datasets into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split ratio = 80:20 -> (441: 111)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_normal = 0\n",
    "for i in y_train:\n",
    "    count_normal += i\n",
    "count_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, model_name):\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1', n_jobs = -1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    best_params =  grid_search.best_params_\n",
    "    best_score =  grid_search.best_score_\n",
    "    # Predicting\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Evaluation\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    return best_model, train_accuracy, test_accuracy, best_params, best_score, train_f1, test_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    # 'Gradient Boosting': (GradientBoostingClassifier(random_state=42),{\n",
    "    #     'loss': ['log_loss', 'exponential'],\n",
    "    #     'criterion': ['friedman_mse', 'squared_error'],\n",
    "    #     'learning_rate': [0.01, 0.1, 0.2, 0.5, 1, 10, 100],\n",
    "    #     'n_estimators': [50, 100, 200, 300, 500],\n",
    "    #     'learning_rate': [0.01, 0.1, 0.2],\n",
    "    #     'max_depth': [3, 5, 7],\n",
    "\n",
    "    # }),\n",
    "\n",
    "    # 'K-Nearest Neighbors': (KNeighborsClassifier(),{\n",
    "    #     'n_neighbors': [1,3,5,7, 9, 11, 13, 17],\n",
    "    #     'leaf_size': [5, 10, 15, 20, 30, 40,  50],\n",
    "    #     'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    #     'weights': ['uniform', 'distance'],\n",
    "    #     'p': [1, 2, 4]\n",
    "    # }),\n",
    "\n",
    "    'XGBClassifier': (XGBClassifier(),{\n",
    "        'n_estimators': [50, 100, 150, 200, 300],  # Number of boosting rounds\n",
    "        'learning_rate': [0.01, 0.1, 0.3, 0.5, 1.0],  # Learning rate\n",
    "        # 'base_estimator__max_depth': [1, 2, 3]  # Depth of the base estimator (Decision Tree)\n",
    "\n",
    "        'alpha': [0, 0.001, 0.1, 1, 10, 100],\n",
    "        # 'max_depth': [3, 5, 7, 9],\n",
    "        # 'learning_rate': [0.1, 0.01, 0.001, 0.2, 0.5, 0.9],\n",
    "        # 'subsample': [0.6, 0.8, 1],\n",
    "        # 'learning_rate': [0.01, 0.1, 0.2],\n",
    "        # 'n_estimators': [100, 200, 300],\n",
    "        # 'max_depth': [3, 5, 7],\n",
    "        # 'min_child_weight': [1, 3, 5],\n",
    "        'gamma': [0, 0.001, 0.1, 1, 10, 100],\n",
    "        'lambda': [0, 0.001, 0.1, 1, 10, 100]\n",
    "        # 'subsample': [0.8, 1.0],\n",
    "        # 'colsample_bytree': [0.8, 1.0]\n",
    "    }),\n",
    "\n",
    "    'Logistic Regression': (LogisticRegression(max_iter = 5000, n_jobs=-1, random_state=42), {\n",
    "        'penalty': ['l1','l2', 'elasticnet'], \n",
    "        'C': [0.001,0.01,0.1,1,10,100,1000],\n",
    "        'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "    }),\n",
    "    # # 'Elastic Net': (ElasticNet(),{}),\n",
    "    'Ridge': (RidgeClassifier(random_state=42, max_iter=5000),{\n",
    "        'solver': [ 'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'],\n",
    "        'alpha': [0.001, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 100, 1000],\n",
    "        'positive': [True, False],\n",
    "        'fit_intercept': [True, False],\n",
    " \n",
    "    }),\n",
    "    # 'Lasso': (Lasso(),{}),\n",
    "    'Extra Trees': (ExtraTreesClassifier(random_state=42, n_jobs=-1),{\n",
    "        'n_estimators': [100, 150, 200, 250, 300], \n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }),\n",
    "    'AdaBoost': (AdaBoostClassifier(random_state=42, algorithm='SAMME'),{\n",
    "        'n_estimators': [50, 70, 90, 120, 160, 180, 200],\n",
    "        'learning_rate': [0.001, 0.01, 0.1 , 0.5, 0.8, 1, 1.5, 5, 10, 100],\n",
    "        'algorithm': ['SAMME', 'SAMME.R']\n",
    "    }),\n",
    "\n",
    "    'Passive Aggressive': (PassiveAggressiveClassifier(max_iter=5000, random_state=42, n_jobs=-1), {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'loss': ['hinge', 'squared_hinge']\n",
    "    }),\n",
    "    'Support Vector Classification': (SVC(random_state=42), {\n",
    "        'C': [0.1, 1, 10, 100, 1000],  \n",
    "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'decision_function_shape': ['ovo', 'ovr']\n",
    "    }),\n",
    "\n",
    "    'Decision Trees': (DecisionTreeClassifier(random_state=42), {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "\n",
    "    }),\n",
    "\n",
    "    'Random Forest': (RandomForestClassifier(random_state=42, n_jobs=-1), {\n",
    "        'n_estimators': [100, 150, 200, 250, 300], \n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }),\n",
    "    # 'Naive Bayes': (GaussianNB(), {'var_smoothing': np.logspace(0,-9, num=100)}),\n",
    "\n",
    "}\n",
    "\n",
    "# {'roc_auc_ovr', 'neg_log_loss', 'neg_median_absolute_error', 'neg_root_mean_squared_log_error', \n",
    "# 'recall_samples', 'recall_micro', 'positive_likelihood_ratio', 'normalized_mutual_info_score', 'f1_samples', \n",
    "# \\'neg_mean_poisson_deviance', 'explained_variance', 'max_error', 'r2', 'v_measure_score', 'accuracy', \n",
    "# 'jaccard_micro', 'average_precision', 'jaccard_macro', 'f1_weighted', 'neg_brier_score', 'rand_score', \n",
    "# 'completeness_score', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'fowlkes_mallows_score', \n",
    "# 'roc_auc', 'adjusted_mutual_info_score', 'homogeneity_score', 'jaccard', 'precision_weighted', \n",
    "# \\'recall_weighted', 'roc_auc_ovo', 'neg_mean_absolute_percentage_error', 'precision_macro', \n",
    "# 'roc_auc_ovr_weighted', 'jaccard_samples', 'precision', 'top_k_accuracy', 'd2_absolute_error_score', \n",
    "# 'matthews_corrcoef', 'roc_auc_ovo_weighted', 'neg_mean_absolute_error', 'f1_micro', 'jaccard_weighted', \n",
    "# 'neg_negative_likelihood_ratio', 'recall_macro', 'balanced_accuracy', 'f1',\n",
    "# 'neg_mean_gamma_deviance', 'mutual_info_score', 'recall', 'neg_root_mean_squared_error', 'f1_macro', 'adjusted_rand_score', 'precision_micro', 'precision_samples'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_models = {}\n",
    "result = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "350 fits failed out of a total of 630.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan 0.                nan        nan        nan 0.\n",
      " 0.         0.36833833 0.         0.         0.         0.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.                nan        nan        nan 0.\n",
      " 0.03405797 0.30052288 0.03405797 0.03405797 0.03405797 0.03405797\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.08818718        nan        nan        nan 0.08946923\n",
      " 0.19520661 0.2642168  0.19732302 0.19732302 0.19732302 0.19732302\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.25715978        nan        nan        nan 0.25676129\n",
      " 0.32721823 0.35130425 0.3377021  0.3377021  0.32721823 0.3254978\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.40714218        nan        nan        nan 0.36359141\n",
      " 0.36121364 0.35791317 0.36121364 0.35907461 0.3668649  0.36217337\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.44852381        nan        nan        nan 0.36428426\n",
      " 0.42707712 0.42707712 0.42707712 0.42707712 0.38098124 0.36428426\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.43264957        nan        nan        nan 0.36428426\n",
      " 0.45590897 0.45280819 0.45786019 0.46925217 0.38774977 0.36428426\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "770 fits failed out of a total of 1760.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 925, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='svd' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 925, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='cholesky' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 925, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='lsqr' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 925, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='sparse_cg' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 925, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='sag' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 925, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='saga' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 918, in fit\n",
      "    raise ValueError(\n",
      "ValueError: 'lbfgs' solver can be used only when positive=True. Please use another solver.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [0.14023755        nan        nan        nan        nan        nan\n",
      "        nan 0.14023755 0.32133801 0.32133801 0.32133801 0.32975863\n",
      " 0.32133801 0.33274882 0.30871309        nan 0.47275669        nan\n",
      "        nan        nan        nan        nan        nan 0.47275669\n",
      " 0.41082009 0.41082009 0.41082009 0.3915186  0.40785635 0.44563835\n",
      " 0.45583906        nan 0.14023755        nan        nan        nan\n",
      "        nan        nan        nan 0.14023755 0.32572825 0.32572825\n",
      " 0.32572825 0.33942641 0.33353313 0.32385993 0.30871309        nan\n",
      " 0.4679973         nan        nan        nan        nan        nan\n",
      "        nan 0.4679973  0.39850186 0.39850186 0.39850186 0.39579751\n",
      " 0.39850186 0.44563835 0.45583906        nan 0.14023755        nan\n",
      "        nan        nan        nan        nan        nan 0.14023755\n",
      " 0.34528472 0.34528472 0.34528472 0.34743525 0.34528472 0.32385993\n",
      " 0.30871309        nan 0.46630508        nan        nan        nan\n",
      "        nan        nan        nan 0.46630508 0.40600894 0.40600894\n",
      " 0.40600894 0.40676612 0.40600894 0.4467799  0.4573309         nan\n",
      " 0.14023755        nan        nan        nan        nan        nan\n",
      "        nan 0.14023755 0.33329433 0.33329433 0.33329433 0.33883707\n",
      " 0.33329433 0.32385993 0.30871309        nan 0.46630508        nan\n",
      "        nan        nan        nan        nan        nan 0.46630508\n",
      " 0.43622719 0.43622719 0.43622719 0.43055165 0.43622719 0.46217057\n",
      " 0.45144925        nan 0.12656233        nan        nan        nan\n",
      "        nan        nan        nan 0.12656233 0.32451147 0.32451147\n",
      " 0.32451147 0.31206285 0.32451147 0.31347699 0.31459322        nan\n",
      " 0.4679973         nan        nan        nan        nan        nan\n",
      "        nan 0.4679973  0.45816005 0.45816005 0.45816005 0.45800016\n",
      " 0.45816005 0.45508914 0.46483265        nan 0.10989567        nan\n",
      "        nan        nan        nan        nan        nan 0.10989567\n",
      " 0.30612405 0.30612405 0.30612405 0.30612405 0.30612405 0.30612405\n",
      " 0.30622465        nan 0.4679973         nan        nan        nan\n",
      "        nan        nan        nan 0.4679973  0.46605839 0.46605839\n",
      " 0.46605839 0.46461608 0.46605839 0.46818103 0.47257085        nan\n",
      " 0.11062031        nan        nan        nan        nan        nan\n",
      "        nan 0.11062031 0.27197724 0.27197724 0.27197724 0.28613108\n",
      " 0.27197724 0.28613108 0.29523564        nan 0.46963665        nan\n",
      "        nan        nan        nan        nan        nan 0.46963665\n",
      " 0.46896469 0.46896469 0.46896469 0.46896469 0.46896469 0.46896469\n",
      " 0.4680747         nan 0.11112644        nan        nan        nan\n",
      "        nan        nan        nan 0.11112644 0.26686992 0.26686992\n",
      " 0.26686992 0.275      0.26686992 0.26686992 0.26686992        nan\n",
      " 0.46963733        nan        nan        nan        nan        nan\n",
      "        nan 0.46963733 0.46956705 0.46956705 0.46956705 0.46956705\n",
      " 0.46956705 0.46956705 0.46924976        nan 0.11234369        nan\n",
      "        nan        nan        nan        nan        nan 0.11234369\n",
      " 0.20534799 0.20534799 0.20534799 0.219593   0.219593   0.20534799\n",
      " 0.20534799        nan 0.47127668        nan        nan        nan\n",
      "        nan        nan        nan 0.47127668 0.46141898 0.46141898\n",
      " 0.46141898 0.46684271 0.46141898 0.46141898 0.46141898        nan\n",
      " 0.08557312        nan        nan        nan        nan        nan\n",
      "        nan 0.08557312 0.14113182 0.14113182 0.14113182 0.14113182\n",
      " 0.14113182 0.1274566  0.1274566         nan 0.47166431        nan\n",
      "        nan        nan        nan        nan        nan 0.47166431\n",
      " 0.46248713 0.46248713 0.46248713 0.46248713 0.46248713 0.46248713\n",
      " 0.46248713        nan 0.01818182        nan        nan        nan\n",
      "        nan        nan        nan 0.01818182 0.0173913  0.0173913\n",
      " 0.0173913  0.0173913  0.0173913  0.0173913  0.0173913         nan\n",
      " 0.44850538        nan        nan        nan        nan        nan\n",
      "        nan 0.44850538 0.42905979 0.42905979 0.42905979 0.42752973\n",
      " 0.42905979 0.42905979 0.42905979        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "2700 fits failed out of a total of 8100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1567 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of ExtraTreesClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1133 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of ExtraTreesClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.07272727 0.07272727 0.05454545]\n",
      "  warnings.warn(\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "1080 fits failed out of a total of 3240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "675 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.48445154 0.44310853 0.42279547 0.39235531 0.40789512 0.36707565\n",
      " 0.41353839 0.36507937 0.42636084 0.40131182 0.4014007  0.35145855\n",
      " 0.43748586 0.34923964 0.43748586 0.34923964 0.37393306 0.27337224\n",
      " 0.45106577 0.46164451 0.39884981 0.34914935 0.39218194 0.32087956\n",
      " 0.36361787 0.26465548 0.37952305 0.32173293 0.35841598 0.19697418\n",
      " 0.34088396 0.16673645 0.34088396 0.16673645 0.37259242 0.13684072\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.43229858 0.24665726 0.40356375 0.27069168 0.40466432 0.31255289\n",
      " 0.42759606 0.22900281 0.43211926 0.29709696 0.44340671 0.23897807\n",
      " 0.44579252 0.30977457 0.44579252 0.30977457 0.38129067 0.23521451\n",
      " 0.33054779 0.25610332 0.34657658 0.23128672 0.33863421 0.15352449\n",
      " 0.25369458 0.2785161  0.32658979 0.26097884 0.31499487 0.16348443\n",
      " 0.34243448 0.16256372 0.34243448 0.16256372 0.31840384 0.11129607\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.48445154 0.40053613 0.42279547 0.37751652 0.40789512 0.36707565\n",
      " 0.41353839 0.32339869 0.42636084 0.39046919 0.4014007  0.37808394\n",
      " 0.43748586 0.34923964 0.43748586 0.34923964 0.37393306 0.27337224\n",
      " 0.40780947 0.44653499 0.39884981 0.37180907 0.39218194 0.27249821\n",
      " 0.36361787 0.2760209  0.37952305 0.31879176 0.35841598 0.19697418\n",
      " 0.34088396 0.16673645 0.34088396 0.16673645 0.37259242 0.13684072\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.48445154 0.44310853 0.42279547 0.39235531 0.40789512 0.36707565\n",
      " 0.41353839 0.36507937 0.42636084 0.40131182 0.4014007  0.35145855\n",
      " 0.43748586 0.34923964 0.43748586 0.34923964 0.37393306 0.27337224\n",
      " 0.43838021 0.46164451 0.39884981 0.34914935 0.39218194 0.32087956\n",
      " 0.36361787 0.26465548 0.37952305 0.32173293 0.35841598 0.19697418\n",
      " 0.34088396 0.16673645 0.34088396 0.16673645 0.37259242 0.13684072\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.46958709 0.44296176 0.4504548  0.39476994 0.46414983 0.32945212\n",
      " 0.39157645 0.41979832 0.40735318 0.284825   0.42365019 0.34578273\n",
      " 0.37762282 0.32133152 0.37762282 0.32133152 0.43341521 0.26494802\n",
      " 0.46630337 0.43999122 0.49179294 0.45266692 0.41995937 0.30096701\n",
      " 0.4029202  0.34250938 0.33643723 0.29320102 0.39898369 0.28789304\n",
      " 0.385531   0.23809648 0.385531   0.23809648 0.40712828 0.21032035\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.40992854 0.26104001 0.42325602 0.29197289 0.41332601 0.26302874\n",
      " 0.41751528 0.27025758 0.41862519 0.25984416 0.4579419  0.2600772\n",
      " 0.45154606 0.29457565 0.45154606 0.29457565 0.37554541 0.27029345\n",
      " 0.33215527 0.21621137 0.24761905 0.27199473 0.23901478 0.20591893\n",
      " 0.26519802 0.28539862 0.21117595 0.26049416 0.24060208 0.31624118\n",
      " 0.36336746 0.13190743 0.36336746 0.13190743 0.35426481 0.13572003\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.49371407 0.44735884 0.46264992 0.40789474 0.46414983 0.31969602\n",
      " 0.39498958 0.41143808 0.39943146 0.29601381 0.42365019 0.34578273\n",
      " 0.37762282 0.32133152 0.37762282 0.32133152 0.43341521 0.26494802\n",
      " 0.42178615 0.45466059 0.4531746  0.43094379 0.40257876 0.26274475\n",
      " 0.4086091  0.27815839 0.34321187 0.34057571 0.35169362 0.28789304\n",
      " 0.38172147 0.20949042 0.38172147 0.20949042 0.40712828 0.21032035\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.46958709 0.44296176 0.4504548  0.39476994 0.46414983 0.32945212\n",
      " 0.39157645 0.41979832 0.40735318 0.284825   0.42365019 0.34578273\n",
      " 0.37762282 0.32133152 0.37762282 0.32133152 0.43341521 0.26494802\n",
      " 0.45373927 0.42344827 0.49179294 0.45266692 0.41995937 0.30096701\n",
      " 0.4029202  0.34250938 0.33643723 0.29320102 0.39898369 0.28789304\n",
      " 0.385531   0.23809648 0.385531   0.23809648 0.40712828 0.21032035\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.46958709 0.44296176 0.4504548  0.39476994 0.46414983 0.32945212\n",
      " 0.39157645 0.41979832 0.40735318 0.284825   0.42365019 0.34578273\n",
      " 0.37762282 0.32133152 0.37762282 0.32133152 0.43341521 0.26494802\n",
      " 0.46630337 0.43999122 0.49179294 0.45266692 0.41995937 0.30096701\n",
      " 0.4029202  0.34250938 0.33643723 0.29320102 0.39898369 0.28789304\n",
      " 0.385531   0.23809648 0.385531   0.23809648 0.40712828 0.21032035\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.40992854 0.26104001 0.42325602 0.29197289 0.41332601 0.26302874\n",
      " 0.41751528 0.27025758 0.41862519 0.25984416 0.4579419  0.2600772\n",
      " 0.45154606 0.29457565 0.45154606 0.29457565 0.37554541 0.27029345\n",
      " 0.33215527 0.21621137 0.24761905 0.27199473 0.23901478 0.20591893\n",
      " 0.26519802 0.28539862 0.21117595 0.26049416 0.24060208 0.31624118\n",
      " 0.36336746 0.13190743 0.36336746 0.13190743 0.35426481 0.13572003\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.49371407 0.44735884 0.46264992 0.40789474 0.46414983 0.31969602\n",
      " 0.39498958 0.41143808 0.39943146 0.29601381 0.42365019 0.34578273\n",
      " 0.37762282 0.32133152 0.37762282 0.32133152 0.43341521 0.26494802\n",
      " 0.42178615 0.45466059 0.4531746  0.43094379 0.40257876 0.26274475\n",
      " 0.4086091  0.27815839 0.34321187 0.34057571 0.35169362 0.28789304\n",
      " 0.38172147 0.20949042 0.38172147 0.20949042 0.40712828 0.21032035\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.46958709 0.44296176 0.4504548  0.39476994 0.46414983 0.32945212\n",
      " 0.39157645 0.41979832 0.40735318 0.284825   0.42365019 0.34578273\n",
      " 0.37762282 0.32133152 0.37762282 0.32133152 0.43341521 0.26494802\n",
      " 0.45373927 0.42344827 0.49179294 0.45266692 0.41995937 0.30096701\n",
      " 0.4029202  0.34250938 0.33643723 0.29320102 0.39898369 0.28789304\n",
      " 0.385531   0.23809648 0.385531   0.23809648 0.40712828 0.21032035]\n",
      "  warnings.warn(\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "2700 fits failed out of a total of 8100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1401 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1299 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.33090162 0.33911276 0.3436798 ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    best_model, train_accuracy, test_accuracy, best_params, best_score, train_f1, test_f1 = train_evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, model_name)\n",
    "    best_models[model_name] = best_model\n",
    "    result[model_name] = [train_accuracy, test_accuracy, train_f1, test_f1, best_score, best_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train F1</th>\n",
       "      <th>Test F1</th>\n",
       "      <th>Best Score</th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.873874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.675551</td>\n",
       "      <td>{'alpha': 0, 'gamma': 0.001, 'lambda': 0, 'lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.891156</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.469252</td>\n",
       "      <td>{'C': 1000, 'penalty': 'l2', 'solver': 'newton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.671202</td>\n",
       "      <td>0.693694</td>\n",
       "      <td>0.521452</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.472757</td>\n",
       "      <td>{'alpha': 0.001, 'fit_intercept': False, 'posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.506845</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 20, 'max_fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.538327</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'learning_rate': 1.5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Passive Aggressive</th>\n",
       "      <td>0.77551</td>\n",
       "      <td>0.774775</td>\n",
       "      <td>0.497462</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.360952</td>\n",
       "      <td>{'C': 0.1, 'loss': 'hinge'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Classification</th>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.828829</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.534743</td>\n",
       "      <td>{'C': 100, 'decision_function_shape': 'ovo', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Trees</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.493714</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 20, 'max...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828829</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.553727</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': None, 'm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Train Accuracy Test Accuracy  Train F1  \\\n",
       "XGBClassifier                            1.0      0.873874       1.0   \n",
       "Logistic Regression                 0.891156      0.783784  0.752577   \n",
       "Ridge                               0.671202      0.693694  0.521452   \n",
       "Extra Trees                              1.0      0.810811       1.0   \n",
       "AdaBoost                                 1.0      0.810811       1.0   \n",
       "Passive Aggressive                   0.77551      0.774775  0.497462   \n",
       "Support Vector Classification       0.968254      0.828829      0.93   \n",
       "Decision Trees                           1.0      0.756757       1.0   \n",
       "Random Forest                            1.0      0.828829       1.0   \n",
       "\n",
       "                                Test F1 Best Score  \\\n",
       "XGBClassifier                  0.681818   0.675551   \n",
       "Logistic Regression            0.538462   0.469252   \n",
       "Ridge                          0.514286   0.472757   \n",
       "Extra Trees                    0.432432   0.506845   \n",
       "AdaBoost                       0.487805   0.538327   \n",
       "Passive Aggressive             0.468085   0.360952   \n",
       "Support Vector Classification  0.612245   0.534743   \n",
       "Decision Trees                 0.470588   0.493714   \n",
       "Random Forest                  0.457143   0.553727   \n",
       "\n",
       "                                                                     Best Params  \n",
       "XGBClassifier                  {'alpha': 0, 'gamma': 0.001, 'lambda': 0, 'lea...  \n",
       "Logistic Regression            {'C': 1000, 'penalty': 'l2', 'solver': 'newton...  \n",
       "Ridge                          {'alpha': 0.001, 'fit_intercept': False, 'posi...  \n",
       "Extra Trees                    {'criterion': 'gini', 'max_depth': 20, 'max_fe...  \n",
       "AdaBoost                       {'algorithm': 'SAMME.R', 'learning_rate': 1.5,...  \n",
       "Passive Aggressive                                   {'C': 0.1, 'loss': 'hinge'}  \n",
       "Support Vector Classification  {'C': 100, 'decision_function_shape': 'ovo', '...  \n",
       "Decision Trees                 {'criterion': 'entropy', 'max_depth': 20, 'max...  \n",
       "Random Forest                  {'criterion': 'entropy', 'max_depth': None, 'm...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(result).T\n",
    "results.columns = ['Train Accuracy', 'Test Accuracy', 'Train F1', 'Test F1', 'Best Score', 'Best Params']\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in models.keys():\n",
    "    # print(results[i])\n",
    "    path = i + \".json\"\n",
    "    results.T[i].to_json(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0,\n",
       " 'gamma': 0.001,\n",
       " 'lambda': 0,\n",
       " 'learning_rate': 1.0,\n",
       " 'n_estimators': 150}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['Best Params'].iloc[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
