{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import ElasticNet, RidgeClassifier, Lasso, PassiveAggressiveClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the uploaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"Data\\\\BP_features.csv\")\n",
    "labels = pd.read_csv(\"Data\\\\final_labels.csv\")\n",
    "labels = labels.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the datasets into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 80:20 -> 441: 111\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_train = StandardScaler()\n",
    "X_train = ss_train.fit_transform(X_train)\n",
    "ss_test = StandardScaler()\n",
    "X_test = ss_test.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to train and evaluate a model\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, model_name):\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy',)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    best_params =  grid_search.best_params_\n",
    "    best_score =  grid_search.best_score_\n",
    "    # Predicting\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Evaluation\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    return best_model, train_accuracy, test_accuracy, best_params, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the models and their hyperparameters\n",
    "\n",
    "models = {\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(),{}),\n",
    "    'K-Nearest Neighbors': (KNeighborsClassifier(),{}),\n",
    "    'XGBClassifier': (XGBClassifier(),{\n",
    "    'alpha': [0.001, 0.01, 0.1, 0.20, 0.25, 0.30],\n",
    "    }),\n",
    "    'Logistic Regression': (LogisticRegression(), {\n",
    "        # 'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]\n",
    "        }),\n",
    "    # 'Elastic Net': (ElasticNet(),{}),\n",
    "    # 'Ridge': (RidgeClassifier(),{}),\n",
    "    # 'Lasso': (Lasso(),{}),\n",
    "    'Extra Trees': (ExtraTreesClassifier(),{}),\n",
    "    'AdaBoost': (AdaBoostClassifier(),{}),\n",
    "    'Passive Aggressive': (PassiveAggressiveClassifier(max_iter=1000, random_state=42, tol=1e-3), {})\n",
    "}\n",
    "\n",
    "\n",
    "# Support Vector Machines\n",
    "from sklearn.svm import LinearSVC\n",
    "models['Support Vector Machines'] = (LinearSVC(), {})\n",
    "\n",
    "# Decision Trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "models['Decision Trees'] = (DecisionTreeClassifier(), {})\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "models['Random Forest'] = (RandomForestClassifier(), {})\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "models['Naive Bayes'] = (GaussianNB(), {})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train and evaluate models for AD8232\n",
    "best_models = {}\n",
    "# results = []\n",
    "result = {}\n",
    "\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    best_model, train_accuracy, test_accuracy, best_params, best_score = train_evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, model_name)\n",
    "    best_models[model_name] = best_model\n",
    "    result[model_name] = [train_accuracy, test_accuracy, best_score, best_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gradient Boosting': [0.981859410430839,\n",
       "  0.8108108108108109,\n",
       "  np.float64(0.8233656792645556),\n",
       "  {}],\n",
       " 'K-Nearest Neighbors': [0.8299319727891157,\n",
       "  0.7387387387387387,\n",
       "  np.float64(0.7530132788559755),\n",
       "  {}],\n",
       " 'XGBClassifier': [1.0,\n",
       "  0.8468468468468469,\n",
       "  np.float64(0.8573289070480081),\n",
       "  {'alpha': 0.3}],\n",
       " 'Logistic Regression': [0.8321995464852607,\n",
       "  0.8018018018018018,\n",
       "  np.float64(0.7440245148110317),\n",
       "  {}],\n",
       " 'Extra Trees': [1.0, 0.8108108108108109, np.float64(0.8164708886619), {}],\n",
       " 'AdaBoost': [0.9047619047619048,\n",
       "  0.8018018018018018,\n",
       "  np.float64(0.7643769152196118),\n",
       "  {}],\n",
       " 'Passive Aggressive': [0.7619047619047619,\n",
       "  0.7387387387387387,\n",
       "  np.float64(0.6849336057201226),\n",
       "  {}],\n",
       " 'Support Vector Machines': [0.854875283446712,\n",
       "  0.7477477477477478,\n",
       "  np.float64(0.7394790602655771),\n",
       "  {}],\n",
       " 'Decision Trees': [1.0,\n",
       "  0.7477477477477478,\n",
       "  np.float64(0.7825331971399387),\n",
       "  {}],\n",
       " 'Random Forest': [1.0,\n",
       "  0.7837837837837838,\n",
       "  np.float64(0.8277834525025536),\n",
       "  {}],\n",
       " 'Naive Bayes': [0.3945578231292517,\n",
       "  0.3153153153153153,\n",
       "  np.float64(0.46726251276813074),\n",
       "  {}]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Best Score</th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.981859</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.823366</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K-Nearest Neighbors</th>\n",
       "      <td>0.829932</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.753013</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846847</td>\n",
       "      <td>0.857329</td>\n",
       "      <td>{'alpha': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.8322</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.744025</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.816471</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.764377</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Passive Aggressive</th>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.684934</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machines</th>\n",
       "      <td>0.854875</td>\n",
       "      <td>0.747748</td>\n",
       "      <td>0.739479</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Trees</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.747748</td>\n",
       "      <td>0.782533</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.827783</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.394558</td>\n",
       "      <td>0.315315</td>\n",
       "      <td>0.467263</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Train Accuracy Test Accuracy Best Score  \\\n",
       "Gradient Boosting             0.981859      0.810811   0.823366   \n",
       "K-Nearest Neighbors           0.829932      0.738739   0.753013   \n",
       "XGBClassifier                      1.0      0.846847   0.857329   \n",
       "Logistic Regression             0.8322      0.801802   0.744025   \n",
       "Extra Trees                        1.0      0.810811   0.816471   \n",
       "AdaBoost                      0.904762      0.801802   0.764377   \n",
       "Passive Aggressive            0.761905      0.738739   0.684934   \n",
       "Support Vector Machines       0.854875      0.747748   0.739479   \n",
       "Decision Trees                     1.0      0.747748   0.782533   \n",
       "Random Forest                      1.0      0.783784   0.827783   \n",
       "Naive Bayes                   0.394558      0.315315   0.467263   \n",
       "\n",
       "                            Best Params  \n",
       "Gradient Boosting                    {}  \n",
       "K-Nearest Neighbors                  {}  \n",
       "XGBClassifier            {'alpha': 0.3}  \n",
       "Logistic Regression                  {}  \n",
       "Extra Trees                          {}  \n",
       "AdaBoost                             {}  \n",
       "Passive Aggressive                   {}  \n",
       "Support Vector Machines              {}  \n",
       "Decision Trees                       {}  \n",
       "Random Forest                        {}  \n",
       "Naive Bayes                          {}  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(result).T\n",
    "results.columns = ['Train Accuracy', 'Test Accuracy', 'Best Score', 'Best Params']\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gradient Boosting': GradientBoostingClassifier(),\n",
       " 'K-Nearest Neighbors': KNeighborsClassifier(),\n",
       " 'XGBClassifier': XGBClassifier(alpha=0.3, base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "               enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "               gamma=None, grow_policy=None, importance_type=None,\n",
       "               interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "               max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "               min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "               multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "               num_parallel_tree=None, ...),\n",
       " 'Logistic Regression': LogisticRegression(),\n",
       " 'Extra Trees': ExtraTreesClassifier(),\n",
       " 'AdaBoost': AdaBoostClassifier(),\n",
       " 'Passive Aggressive': PassiveAggressiveClassifier(random_state=42),\n",
       " 'Support Vector Machines': LinearSVC(),\n",
       " 'Decision Trees': DecisionTreeClassifier(),\n",
       " 'Random Forest': RandomForestClassifier(),\n",
       " 'Naive Bayes': GaussianNB()}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
