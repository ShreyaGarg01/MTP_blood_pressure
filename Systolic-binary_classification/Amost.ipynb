{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "236G9s8kP-Po"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        ")\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import (\n",
        "    GradientBoostingClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    RandomForestClassifier,\n",
        ")\n",
        "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import StackingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g-E1QOyP-Pp"
      },
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-Yk4ppkOP-Pq"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"..\\\\Data\\\\BP_features.csv\")\n",
        "labels = pd.read_csv(\"..\\\\Data\\\\BP_labels.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "faksxCkJj4cB",
        "outputId": "5a52fd4d-b329-40b5-a1c4-a3e86f62c5a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      120\n",
              "1      140\n",
              "2      150\n",
              "3      130\n",
              "4      110\n",
              "      ... \n",
              "547    140\n",
              "548    116\n",
              "549    140\n",
              "550    114\n",
              "551    127\n",
              "Name: 0, Length: 552, dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels[\"0\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UhxinPq3i0YY"
      },
      "outputs": [],
      "source": [
        "def get_binary_class(value):\n",
        "    # Normal Class for Max_BP (90-140)\n",
        "    if value >= 90 and value <= 140:\n",
        "        return 0\n",
        "    # Abnormal Class for Max_BP (<90) or (>140)\n",
        "    return 1\n",
        "\n",
        "labels = labels[\"0\"].apply(lambda x: get_binary_class(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "Znrgy7_wkUs7",
        "outputId": "afe77959-489b-4118-f383-8f6dfe26d965"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0\n",
              "0    424\n",
              "1    128\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = np.ravel(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjsecDl4P-Pr"
      },
      "source": [
        "### Splitting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mu049gpRP-Pr"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, labels, test_size=0.2, random_state=42, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_p0BwM4_20H",
        "outputId": "96fe9833-b461-43fc-a476-8b3e29be80d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(441,)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7cGfO0KP-Pr"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zjxa4HWzP-Pr"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w7rwcrwP-Pr"
      },
      "source": [
        "### Train and evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_evaluate_model(X_train, X_test, y_train, y_test, model, param_grid):\n",
        "\n",
        "    bayes_search = BayesSearchCV(\n",
        "        model,\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        scoring=\"f1_macro\",\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        n_iter=20,\n",
        "    )\n",
        "    bayes_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = bayes_search.best_estimator_\n",
        "    best_params = bayes_search.best_params_\n",
        "\n",
        "    y_train_pred = best_model.predict(X_train)\n",
        "    y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Evaluation Metrics!\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    train_f1 = f1_score(y_train, y_train_pred)\n",
        "    test_f1 = f1_score(y_test, y_test_pred)\n",
        "\n",
        "    try:\n",
        "        train_roc = roc_auc_score(y_train, bayes_search.predict_proba(X_train)[:, 1])\n",
        "        test_roc = roc_auc_score(y_test, bayes_search.predict_proba(X_test)[:, 1])\n",
        "        train_roc = np.round(train_roc * 100, 2)\n",
        "        test_roc = np.round(test_roc * 100, 2)\n",
        "\n",
        "    except:\n",
        "        train_roc = \"Not found\"\n",
        "        test_roc = \"Not found\"\n",
        "\n",
        "    return (\n",
        "        best_model,\n",
        "        best_params,\n",
        "        train_accuracy,\n",
        "        test_accuracy,\n",
        "        train_f1,\n",
        "        test_f1,\n",
        "        train_roc,\n",
        "        test_roc,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parameter Grid for different models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"Logistic Regression\": [\n",
        "        LogisticRegression(\n",
        "            max_iter=2000, n_jobs=-1, random_state=42, class_weight={0: 1, 1: 3}\n",
        "        ),\n",
        "        {\n",
        "            \"C\": Real(0.01, 10, prior=\"log-uniform\"),\n",
        "            \"fit_intercept\": [True, False],\n",
        "            \"penalty\": [\"l2\", None],\n",
        "            \"solver\": [\"lbfgs\", \"newton-cg\", \"newton-cholesky\", \"sag\"],\n",
        "            \"tol\": Real(1e-5, 1e-3, prior=\"log-uniform\"),\n",
        "        },\n",
        "    ],\n",
        "    \"Ridge Regression\": [\n",
        "        RidgeClassifier(random_state=42, max_iter=2000, class_weight={0: 1, 1: 3.24}),\n",
        "        {\n",
        "            \"alpha\": Real(0.1, 100, prior=\"log-uniform\"),\n",
        "            \"fit_intercept\": [True, False],\n",
        "        },\n",
        "    ],\n",
        "    \"SVM\": [\n",
        "        SVC(random_state=42, class_weight=\"balanced\", probability=True),\n",
        "        {\n",
        "            \"gamma\": Categorical([\"scale\", \"auto\"]),\n",
        "            \"C\": Real(5e-3, 500, prior=\"log-uniform\"),\n",
        "            \"kernel\": Categorical([\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
        "        },\n",
        "    ],\n",
        "    \"Decision Tree\": [\n",
        "        DecisionTreeClassifier(random_state=42),\n",
        "        {\n",
        "            \"max_depth\": Integer(3, 10),\n",
        "            \"criterion\": Categorical([\"gini\", \"entropy\", \"log_loss\"]),\n",
        "            \"min_samples_split\": Integer(2, 10),\n",
        "            \"min_samples_leaf\": Integer(2, 10),\n",
        "        },\n",
        "    ],\n",
        "    \"KNN\": [\n",
        "        KNeighborsClassifier(),\n",
        "        {\n",
        "            \"metric\": Categorical([\"euclidean\", \"manhattan\"]),\n",
        "            \"n_neighbors\": Integer(3, 30),\n",
        "            \"weights\": Categorical([\"uniform\", \"distance\"]),\n",
        "            \"algorithm\": Categorical([\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]),\n",
        "            \"p\": Integer(1, 2),\n",
        "        },\n",
        "    ],\n",
        "    \"AdaBoost\": [\n",
        "        AdaBoostClassifier(random_state=42, algorithm=\"SAMME\"),\n",
        "        {\n",
        "            \"n_estimators\": Integer(50, 210),\n",
        "            \"learning_rate\": Real(0.001, 1, prior=\"log-uniform\"),\n",
        "        },\n",
        "    ],\n",
        "    \"Random Forest\": [\n",
        "        RandomForestClassifier(random_state=42, n_jobs=-1, class_weight=\"balanced\"),\n",
        "        {\n",
        "            \"n_estimators\": Integer(50, 500),\n",
        "            \"max_depth\": Integer(3, 20),\n",
        "            \"min_samples_split\": Integer(2, 100),\n",
        "            \"min_samples_leaf\": Integer(1, 50),\n",
        "            \"max_features\": Categorical([\"sqrt\", \"log2\", None]),\n",
        "            \"bootstrap\": Categorical([True, False]),\n",
        "            \"criterion\": Categorical([\"gini\", \"entropy\", \"log_loss\"]),\n",
        "        },\n",
        "    ],\n",
        "    \"Gradient Boosting\": [\n",
        "        GradientBoostingClassifier(random_state=42),\n",
        "        {\n",
        "            \"criterion\": Categorical([\"friedman_mse\", \"squared_error\"]),\n",
        "            \"n_estimators\": Integer(50, 300),\n",
        "            \"learning_rate\": Real(1e-4, 0.01, prior=\"log-uniform\"),\n",
        "            \"max_depth\": Integer(3, 8),\n",
        "            \"min_samples_split\": Integer(10, 50),\n",
        "            \"min_samples_leaf\": Integer(5, 30),\n",
        "            \"n_iter_no_change\": Integer(5, 20),\n",
        "        },\n",
        "    ],\n",
        "    \"XGBoost\": [\n",
        "        XGBClassifier(n_jobs=-1, random_state=42),\n",
        "        {\n",
        "            \"n_estimators\": Integer(100, 500),\n",
        "            \"learning_rate\": Real(0.001, 0.05, prior=\"log-uniform\"),\n",
        "            \"max_depth\": Integer(3, 10),\n",
        "            \"colsample_bytree\": Real(0.6, 0.9),\n",
        "            \"gamma\": Real(1, 5, prior=\"log-uniform\"),\n",
        "            \"reg_alpha\": Real(1, 5, prior=\"log-uniform\"),\n",
        "            \"reg_lambda\": Real(1, 10, prior=\"log-uniform\"),\n",
        "            \"min_child_weight\": Integer(3, 10),\n",
        "            \"scale_pos_weight\": Real(1, 10),\n",
        "        },\n",
        "    ],\n",
        "    \"Naive Bayes\": (\n",
        "        GaussianNB(),\n",
        "        {\"var_smoothing\": Real(1e-6, 1e-2, prior=\"log-uniform\")},\n",
        "    ),\n",
        "    \"BernoulliNB\": (BernoulliNB(), {\"alpha\": Real(0.001, 1, prior=\"log-uniform\")}),\n",
        "    \"Light GBM\": [\n",
        "        LGBMClassifier(\n",
        "            n_jobs=-1, random_state=42, class_weight=\"balanced\", verbosity=-1\n",
        "        ),\n",
        "        {\n",
        "            \"reg_alpha\": Real(1, 10, prior=\"log-uniform\"),\n",
        "            \"reg_lambda\": Real(1, 10, prior=\"log-uniform\"),\n",
        "            \"num_leaves\": Integer(20, 50),\n",
        "            \"max_depth\": Integer(3, 8),\n",
        "            \"learning_rate\": Real(1e-4, 1e-2, prior=\"log-uniform\"),\n",
        "            \"n_estimators\": Integer(100, 500),\n",
        "            \"min_child_samples\": Integer(50, 100),\n",
        "            \"subsample\": Real(0.6, 1.0),\n",
        "            \"colsample_bytree\": Real(0.5, 0.8),\n",
        "        },\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_models = {}\n",
        "results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Logistic Regression\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Model': 'Logistic Regression', 'Train Accuracy': 85.26, 'Test Accuracy': 76.58, 'Train F1': 73.68, 'Test F1': 59.38, 'Train ROC': 93.17, 'Test ROC': 76.2}\n",
            "Model: Ridge Regression\n",
            "{'Model': 'Ridge Regression', 'Train Accuracy': 74.6, 'Test Accuracy': 74.77, 'Train F1': 58.21, 'Test F1': 56.25, 'Train ROC': 'Not found', 'Test ROC': 'Not found'}\n",
            "Model: SVM\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [499.99999999999994, 'auto', 'rbf'] before, using random point [283.7611750312267, 'scale', 'sigmoid']\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Model': 'SVM', 'Train Accuracy': 86.85, 'Test Accuracy': 81.98, 'Train F1': 77.17, 'Test F1': 65.52, 'Train ROC': 94.59, 'Test ROC': 84.1}\n",
            "Model: Decision Tree\n",
            "{'Model': 'Decision Tree', 'Train Accuracy': 95.01, 'Test Accuracy': 83.78, 'Train F1': 89.11, 'Test F1': 65.38, 'Train ROC': 98.95, 'Test ROC': 84.41}\n",
            "Model: KNN\n",
            "{'Model': 'KNN', 'Train Accuracy': 100.0, 'Test Accuracy': 82.88, 'Train F1': 100.0, 'Test F1': 55.81, 'Train ROC': 100.0, 'Test ROC': 82.85}\n",
            "Model: AdaBoost\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [1.0, 210] before, using random point [0.008242236174964803, 71]\n",
            "  warnings.warn(\n",
            "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [1.0, 210] before, using random point [0.2812916994233041, 202]\n",
            "  warnings.warn(\n",
            "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [1.0, 210] before, using random point [0.00947422621673148, 85]\n",
            "  warnings.warn(\n",
            "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [1.0, 210] before, using random point [0.011983153296448057, 51]\n",
            "  warnings.warn(\n",
            "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [1.0, 210] before, using random point [0.22062572896371738, 167]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Model': 'AdaBoost', 'Train Accuracy': 91.16, 'Test Accuracy': 90.09, 'Train F1': 79.37, 'Test F1': 73.17, 'Train ROC': 97.4, 'Test ROC': 88.55}\n",
            "Model: Random Forest\n",
            "{'Model': 'Random Forest', 'Train Accuracy': 92.74, 'Test Accuracy': 85.59, 'Train F1': 85.59, 'Test F1': 68.0, 'Train ROC': 97.99, 'Test ROC': 85.06}\n",
            "Model: Gradient Boosting\n",
            "{'Model': 'Gradient Boosting', 'Train Accuracy': 96.15, 'Test Accuracy': 87.39, 'Train F1': 91.19, 'Test F1': 65.0, 'Train ROC': 98.45, 'Test ROC': 84.51}\n",
            "Model: XGBoost\n",
            "{'Model': 'XGBoost', 'Train Accuracy': 87.3, 'Test Accuracy': 86.49, 'Train F1': 78.29, 'Test F1': 70.59, 'Train ROC': 96.19, 'Test ROC': 84.96}\n",
            "Model: Naive Bayes\n",
            "{'Model': 'Naive Bayes', 'Train Accuracy': 67.12, 'Test Accuracy': 66.67, 'Train F1': 40.82, 'Test F1': 32.73, 'Train ROC': 65.99, 'Test ROC': 54.5}\n",
            "Model: BernoulliNB\n",
            "{'Model': 'BernoulliNB', 'Train Accuracy': 72.79, 'Test Accuracy': 72.07, 'Train F1': 41.18, 'Test F1': 39.22, 'Train ROC': 67.01, 'Test ROC': 64.03}\n",
            "Model: Light GBM\n",
            "{'Model': 'Light GBM', 'Train Accuracy': 87.53, 'Test Accuracy': 86.49, 'Train F1': 77.91, 'Test F1': 69.39, 'Train ROC': 95.74, 'Test ROC': 84.67}\n"
          ]
        }
      ],
      "source": [
        "for model_name, (model, param_grid) in models.items():\n",
        "    print(\"Model:\", model_name)\n",
        "    (\n",
        "        best_model,\n",
        "        best_params,\n",
        "        train_accuracy,\n",
        "        test_accuracy,\n",
        "        train_f1,\n",
        "        test_f1,\n",
        "        train_roc,\n",
        "        test_roc,\n",
        "    ) = train_evaluate_model(X_train, X_test, y_train, y_test, model, param_grid)\n",
        "    best_models[model_name] = best_model\n",
        "    item = {\n",
        "        \"Model\": model_name,\n",
        "        \"Train Accuracy\": np.round(train_accuracy * 100, 2),\n",
        "        \"Test Accuracy\": np.round(test_accuracy * 100, 2),\n",
        "        \"Train F1\": np.round(train_f1 * 100, 2),\n",
        "        \"Test F1\": np.round(test_f1 * 100, 2),\n",
        "        \"Train ROC\": train_roc,\n",
        "        \"Test ROC\": test_roc,\n",
        "        # \"Best Parameters\": best_params,\n",
        "    }\n",
        "    print(item)\n",
        "    results.append(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Train Accuracy</th>\n",
              "      <th>Test Accuracy</th>\n",
              "      <th>Train F1</th>\n",
              "      <th>Test F1</th>\n",
              "      <th>Train ROC</th>\n",
              "      <th>Test ROC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>85.26</td>\n",
              "      <td>76.58</td>\n",
              "      <td>73.68</td>\n",
              "      <td>59.38</td>\n",
              "      <td>93.17</td>\n",
              "      <td>76.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ridge Regression</td>\n",
              "      <td>74.60</td>\n",
              "      <td>74.77</td>\n",
              "      <td>58.21</td>\n",
              "      <td>56.25</td>\n",
              "      <td>Not found</td>\n",
              "      <td>Not found</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SVM</td>\n",
              "      <td>86.85</td>\n",
              "      <td>81.98</td>\n",
              "      <td>77.17</td>\n",
              "      <td>65.52</td>\n",
              "      <td>94.59</td>\n",
              "      <td>84.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decision Tree</td>\n",
              "      <td>95.01</td>\n",
              "      <td>83.78</td>\n",
              "      <td>89.11</td>\n",
              "      <td>65.38</td>\n",
              "      <td>98.95</td>\n",
              "      <td>84.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KNN</td>\n",
              "      <td>100.00</td>\n",
              "      <td>82.88</td>\n",
              "      <td>100.00</td>\n",
              "      <td>55.81</td>\n",
              "      <td>100.0</td>\n",
              "      <td>82.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>91.16</td>\n",
              "      <td>90.09</td>\n",
              "      <td>79.37</td>\n",
              "      <td>73.17</td>\n",
              "      <td>97.4</td>\n",
              "      <td>88.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>92.74</td>\n",
              "      <td>85.59</td>\n",
              "      <td>85.59</td>\n",
              "      <td>68.00</td>\n",
              "      <td>97.99</td>\n",
              "      <td>85.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>96.15</td>\n",
              "      <td>87.39</td>\n",
              "      <td>91.19</td>\n",
              "      <td>65.00</td>\n",
              "      <td>98.45</td>\n",
              "      <td>84.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>87.30</td>\n",
              "      <td>86.49</td>\n",
              "      <td>78.29</td>\n",
              "      <td>70.59</td>\n",
              "      <td>96.19</td>\n",
              "      <td>84.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>67.12</td>\n",
              "      <td>66.67</td>\n",
              "      <td>40.82</td>\n",
              "      <td>32.73</td>\n",
              "      <td>65.99</td>\n",
              "      <td>54.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>BernoulliNB</td>\n",
              "      <td>72.79</td>\n",
              "      <td>72.07</td>\n",
              "      <td>41.18</td>\n",
              "      <td>39.22</td>\n",
              "      <td>67.01</td>\n",
              "      <td>64.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Light GBM</td>\n",
              "      <td>87.53</td>\n",
              "      <td>86.49</td>\n",
              "      <td>77.91</td>\n",
              "      <td>69.39</td>\n",
              "      <td>95.74</td>\n",
              "      <td>84.67</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Model  Train Accuracy  Test Accuracy  Train F1  Test F1  \\\n",
              "0   Logistic Regression           85.26          76.58     73.68    59.38   \n",
              "1      Ridge Regression           74.60          74.77     58.21    56.25   \n",
              "2                   SVM           86.85          81.98     77.17    65.52   \n",
              "3         Decision Tree           95.01          83.78     89.11    65.38   \n",
              "4                   KNN          100.00          82.88    100.00    55.81   \n",
              "5              AdaBoost           91.16          90.09     79.37    73.17   \n",
              "6         Random Forest           92.74          85.59     85.59    68.00   \n",
              "7     Gradient Boosting           96.15          87.39     91.19    65.00   \n",
              "8               XGBoost           87.30          86.49     78.29    70.59   \n",
              "9           Naive Bayes           67.12          66.67     40.82    32.73   \n",
              "10          BernoulliNB           72.79          72.07     41.18    39.22   \n",
              "11            Light GBM           87.53          86.49     77.91    69.39   \n",
              "\n",
              "    Train ROC   Test ROC  \n",
              "0       93.17       76.2  \n",
              "1   Not found  Not found  \n",
              "2       94.59       84.1  \n",
              "3       98.95      84.41  \n",
              "4       100.0      82.85  \n",
              "5        97.4      88.55  \n",
              "6       97.99      85.06  \n",
              "7       98.45      84.51  \n",
              "8       96.19      84.96  \n",
              "9       65.99       54.5  \n",
              "10      67.01      64.03  \n",
              "11      95.74      84.67  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = pd.DataFrame(results)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ensemble\n",
        "\n",
        "#### Stacking Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Train Accuracy</th>\n",
              "      <td>89.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train F1 Score</th>\n",
              "      <td>81.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train ROC AUC Score</th>\n",
              "      <td>98.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test Accuracy</th>\n",
              "      <td>89.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test F1 Score</th>\n",
              "      <td>76.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test ROC AUC Score</th>\n",
              "      <td>86.78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         0\n",
              "Train Accuracy       89.34\n",
              "Train F1 Score       81.42\n",
              "Train ROC AUC Score  98.10\n",
              "Test Accuracy        89.19\n",
              "Test F1 Score        76.92\n",
              "Test ROC AUC Score   86.78"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_models = [\n",
        "    \"XGBoost\",\n",
        "    \"Light GBM\",\n",
        "    \"SVM\",\n",
        "    \"Decision Tree\",\n",
        "    \"AdaBoost\",\n",
        "    \"Naive Bayes\",\n",
        "]\n",
        "\n",
        "meta_model = best_models[\"Logistic Regression\"]\n",
        "evaluation_results2 = []\n",
        "base = []\n",
        "for model in base_models:\n",
        "    base.append((model, best_models[model]))\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators=base,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    stack_method=\"predict_proba\",\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "\n",
        "train_acc = accuracy_score(y_train, stack.predict(X_train)) * 100\n",
        "train_f1 = f1_score(y_train, stack.predict(X_train)) * 100\n",
        "train_roc = roc_auc_score(y_train, stack.predict_proba(X_train)[: , 1]) * 100\n",
        "\n",
        "test_acc = accuracy_score(y_test, stack.predict(X_test)) * 100\n",
        "test_f1 = f1_score(y_test, stack.predict(X_test)) * 100\n",
        "test_roc = roc_auc_score(y_test, stack.predict_proba(X_test)[: , 1]) * 100\n",
        "\n",
        "evaluation_results2.append(\n",
        "    {\n",
        "        \"Train Accuracy\": np.round(train_acc, 2),\n",
        "        \"Train F1 Score\": np.round(train_f1, 2),\n",
        "        \"Train ROC AUC Score\": np.round(train_roc, 2),\n",
        "        \"Test Accuracy\": np.round(test_acc, 2),\n",
        "        \"Test F1 Score\": np.round(test_f1, 2),\n",
        "        \"Test ROC AUC Score\": np.round(test_roc, 2),  \n",
        "    }\n",
        ")\n",
        "\n",
        "results2 = pd.DataFrame(data=evaluation_results2).T\n",
        "results2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Train Accuracy</th>\n",
              "      <td>94.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train F1 Score</th>\n",
              "      <td>89.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train ROC AUC Score</th>\n",
              "      <td>98.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test Accuracy</th>\n",
              "      <td>89.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test F1 Score</th>\n",
              "      <td>76.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test ROC AUC Score</th>\n",
              "      <td>87.31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         0\n",
              "Train Accuracy       94.56\n",
              "Train F1 Score       89.09\n",
              "Train ROC AUC Score  98.83\n",
              "Test Accuracy        89.19\n",
              "Test F1 Score        76.00\n",
              "Test ROC AUC Score   87.31"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "model_names = [\n",
        "    \"XGBoost\",\n",
        "    \"Light GBM\",\n",
        "    \"Decision Tree\",\n",
        "    \"AdaBoost\",\n",
        "    \"Naive Bayes\",\n",
        "    \"BernoulliNB\",\n",
        "    \"Logistic Regression\",\n",
        "]\n",
        "\n",
        "evaluation_results2 = []\n",
        "base = []\n",
        "for j in range(len(model_names)):\n",
        "    base.append((model_names[j], best_models[model_names[j]]))\n",
        "\n",
        "stack = VotingClassifier(\n",
        "    estimators=base,\n",
        "    n_jobs=-1,\n",
        "    voting=\"soft\",\n",
        "    weights=[1, 1, 1, 3, 0.2, 0.3, 0.8],\n",
        ")\n",
        "stack.fit(X_train, y_train)\n",
        "\n",
        "train_acc = accuracy_score(y_train, stack.predict(X_train)) * 100 \n",
        "train_f1 = f1_score(y_train, stack.predict(X_train)) * 100\n",
        "train_roc = roc_auc_score(y_train, stack.predict_proba(X_train)[: , 1]) * 100\n",
        "test_acc = accuracy_score(y_test, stack.predict(X_test)) * 100\n",
        "test_f1 = f1_score(y_test, stack.predict(X_test)) * 100\n",
        "test_roc = roc_auc_score(y_test, stack.predict_proba(X_test)[: , 1]) * 100\n",
        "\n",
        "evaluation_results2.append(\n",
        "    {\n",
        "        \"Train Accuracy\": np.round(train_acc, 2),\n",
        "        \"Train F1 Score\": np.round(train_f1, 2),\n",
        "        \"Train ROC AUC Score\": np.round(train_roc, 2),\n",
        "        \"Test Accuracy\": np.round(test_acc, 2),\n",
        "        \"Test F1 Score\": np.round(test_f1, 2),\n",
        "        \"Test ROC AUC Score\": np.round(test_roc, 2),  \n",
        "    }\n",
        ")\n",
        "\n",
        "results2 = pd.DataFrame(data=evaluation_results2).T\n",
        "results2\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
