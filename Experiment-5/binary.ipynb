{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import ElasticNet, RidgeClassifier, Lasso, PassiveAggressiveClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from hyperopt import hp\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the uploaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"..\\\\Data\\\\BP_features.csv\")\n",
    "labels = pd.read_csv(\"..\\\\Data\\\\final_labels.csv\")\n",
    "labels = labels.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_normal = 0\n",
    "for i in labels:\n",
    "    count_normal += i\n",
    "count_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the datasets into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split ratio = 80:20 -> (441: 111)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.25, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_normal = 0\n",
    "for i in y_train:\n",
    "    count_normal += i\n",
    "count_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, model_name):\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc', n_jobs = -1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    best_params =  grid_search.best_params_\n",
    "    best_score =  grid_search.best_score_\n",
    "    # Predicting\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Evaluation\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    return best_model, train_accuracy, test_accuracy, best_params, best_score, train_f1, test_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    'Naive Bayes': (GaussianNB(), {'var_smoothing': np.logspace(0,-9, num=100)}),\n",
    "\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(random_state=42),{\n",
    "        'loss': ['log_loss', 'exponential'],\n",
    "        'criterion': ['friedman_mse', 'squared_error'],\n",
    "        'learning_rate': [0.01, 0.1, 0.2, 0.5, 1, 10, 100],\n",
    "        'n_estimators': [50, 100, 200, 300, 500],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "\n",
    "    }),\n",
    "\n",
    "    'K-Nearest Neighbors': (KNeighborsClassifier(),{\n",
    "        'n_neighbors': [1,3,5,7, 9, 11, 13, 17],\n",
    "        'leaf_size': [5, 10, 15, 20, 30, 40,  50],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2, 4]\n",
    "    }),\n",
    "\n",
    "    'Logistic Regression': (LogisticRegression(max_iter = 5000, n_jobs=-1, random_state=42), {\n",
    "        'penalty': ['l1','l2', 'elasticnet'], \n",
    "        'C': [0.001,0.01,0.1,1,10,100,1000],\n",
    "        'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "    }),\n",
    "    # # 'Elastic Net': (ElasticNet(),{}),\n",
    "    'Ridge': (RidgeClassifier(random_state=42, max_iter=5000),{\n",
    "        'solver': [ 'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'],\n",
    "        'alpha': [0.001, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 100, 1000],\n",
    "        'positive': [True, False],\n",
    "        'fit_intercept': [True, False],\n",
    " \n",
    "    }),\n",
    "    # 'Lasso': (Lasso(),{}),\n",
    "    'Extra Trees': (ExtraTreesClassifier(random_state=42, n_jobs=-1),{\n",
    "        'n_estimators': [100, 150, 200, 250, 300], \n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }),\n",
    "    'AdaBoost': (AdaBoostClassifier(random_state=42, algorithm='SAMME'),{\n",
    "        'n_estimators': [50, 70, 90, 120, 160, 180, 200],\n",
    "        'learning_rate': [0.001, 0.01, 0.1 , 0.5, 0.8, 1, 1.5, 5, 10, 100],\n",
    "        'algorithm': ['SAMME', 'SAMME.R']\n",
    "    }),\n",
    "\n",
    "    'Passive Aggressive': (PassiveAggressiveClassifier(max_iter=5000, random_state=42, n_jobs=-1), {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'loss': ['hinge', 'squared_hinge']\n",
    "    }),\n",
    "    'Support Vector Classification': (SVC(random_state=42), {\n",
    "        'C': [0.1, 1, 10, 100, 1000],  \n",
    "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'decision_function_shape': ['ovo', 'ovr']\n",
    "    }),\n",
    "\n",
    "    'Decision Trees': (DecisionTreeClassifier(random_state=42), {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "\n",
    "    }),\n",
    "\n",
    "    'Random Forest': (RandomForestClassifier(random_state=42, n_jobs=-1), {\n",
    "        'n_estimators': [100, 150, 200, 250, 300], \n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }),\n",
    "    'XGBClassifier': (XGBClassifier(),{\n",
    "        'n_estimators': [50, 100, 150, 200, 300],  # Number of boosting rounds\n",
    "        'learning_rate': [0.01, 0.1, 0.3, 0.5, 1.0],  # Learning rate\n",
    "        # 'base_estimator__max_depth': [1, 2, 3]  # Depth of the base estimator (Decision Tree)\n",
    "\n",
    "        'alpha': [0, 0.001, 0.1, 1, 10, 100],\n",
    "        # 'max_depth': [3, 5, 7, 9],\n",
    "        # 'learning_rate': [0.1, 0.01, 0.001, 0.2, 0.5, 0.9],\n",
    "        # 'subsample': [0.6, 0.8, 1],\n",
    "        # 'learning_rate': [0.01, 0.1, 0.2],\n",
    "        # 'n_estimators': [100, 200, 300],\n",
    "        # 'max_depth': [3, 5, 7],\n",
    "        # 'min_child_weight': [1, 3, 5],\n",
    "        'gamma': [0, 0.001, 0.1, 1, 10, 100],\n",
    "        'lambda': [0, 0.001, 0.1, 1, 10, 100]\n",
    "        # 'subsample': [0.8, 1.0],\n",
    "        # 'colsample_bytree': [0.8, 1.0]\n",
    "    }),\n",
    "\n",
    "}\n",
    "\n",
    "# {'roc_auc_ovr', 'neg_log_loss', 'neg_median_absolute_error', 'neg_root_mean_squared_log_error', \n",
    "# 'recall_samples', 'recall_micro', 'positive_likelihood_ratio', 'normalized_mutual_info_score', 'f1_samples', \n",
    "# \\'neg_mean_poisson_deviance', 'explained_variance', 'max_error', 'r2', 'v_measure_score', 'accuracy', \n",
    "# 'jaccard_micro', 'average_precision', 'jaccard_macro', 'f1_weighted', 'neg_brier_score', 'rand_score', \n",
    "# 'completeness_score', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'fowlkes_mallows_score', \n",
    "# 'roc_auc', 'adjusted_mutual_info_score', 'homogeneity_score', 'jaccard', 'precision_weighted', \n",
    "# \\'recall_weighted', 'roc_auc_ovo', 'neg_mean_absolute_percentage_error', 'precision_macro', \n",
    "# 'roc_auc_ovr_weighted', 'jaccard_samples', 'precision', 'top_k_accuracy', 'd2_absolute_error_score', \n",
    "# 'matthews_corrcoef', 'roc_auc_ovo_weighted', 'neg_mean_absolute_error', 'f1_micro', 'jaccard_weighted', \n",
    "# 'neg_negative_likelihood_ratio', 'recall_macro', 'balanced_accuracy', 'f1',\n",
    "# 'neg_mean_gamma_deviance', 'mutual_info_score', 'recall', 'neg_root_mean_squared_error', 'f1_macro', 'adjusted_rand_score', 'precision_micro', 'precision_samples'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_models = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "350 fits failed out of a total of 630.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan 0.5               nan        nan        nan 0.5\n",
      " 0.6214599  0.63097013 0.6214599  0.6214599  0.6214599  0.6214599\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.5               nan        nan        nan 0.5\n",
      " 0.65292972 0.6756981  0.65276525 0.65292398 0.65309106 0.65324979\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.71961414        nan        nan        nan 0.71980681\n",
      " 0.68883354 0.70174655 0.68866646 0.68850198 0.68849363 0.68896982\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.69229376        nan        nan        nan 0.69426483\n",
      " 0.70628394 0.71712093 0.70644267 0.70723632 0.70644267 0.70437082\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.69646878        nan        nan        nan 0.70365288\n",
      " 0.68906067 0.6912949  0.68907216 0.68907216 0.69427423 0.69711257\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.65905858        nan        nan        nan 0.69384555\n",
      " 0.68244883 0.68372442 0.68310359 0.68247128 0.68595656 0.69320228\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.65134398        nan        nan        nan 0.69208542\n",
      " 0.66093202 0.65960474 0.65978906 0.6591202  0.6854553  0.69208542\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "770 fits failed out of a total of 1760.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 925, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='svd' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 925, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='cholesky' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 925, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='lsqr' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 925, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='sparse_cg' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 925, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='sag' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 925, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='saga' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "110 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1565, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 918, in fit\n",
      "    raise ValueError(\n",
      "ValueError: 'lbfgs' solver can be used only when positive=True. Please use another solver.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [0.69630796        nan        nan        nan        nan        nan\n",
      "        nan 0.69630796 0.69278143 0.69278143 0.69278143 0.69652882\n",
      " 0.69086779 0.71210526 0.71166197        nan 0.6964881         nan\n",
      "        nan        nan        nan        nan        nan 0.6964881\n",
      " 0.67702694 0.67702694 0.67702694 0.67511696 0.67657581 0.69638732\n",
      " 0.70457498        nan 0.69441155        nan        nan        nan\n",
      "        nan        nan        nan 0.69441155 0.70139411 0.70139411\n",
      " 0.70139411 0.70403509 0.70121815 0.71227235 0.71166197        nan\n",
      " 0.69712302        nan        nan        nan        nan        nan\n",
      "        nan 0.69712302 0.6831156  0.6831156  0.6831156  0.67871554\n",
      " 0.68294538 0.69589442 0.70457498        nan 0.69360067        nan\n",
      "        nan        nan        nan        nan        nan 0.69360067\n",
      " 0.71163586 0.71163586 0.71163586 0.71313492 0.71245771 0.71191834\n",
      " 0.71214129        nan 0.69664683        nan        nan        nan\n",
      "        nan        nan        nan 0.69664683 0.69163012 0.69163012\n",
      " 0.69163012 0.69455201 0.69178885 0.6975188  0.70552736        nan\n",
      " 0.69424708        nan        nan        nan        nan        nan\n",
      "        nan 0.69424708 0.71673246 0.71673246 0.71673246 0.71972483\n",
      " 0.71674916 0.71370405 0.71195123        nan 0.69696429        nan\n",
      "        nan        nan        nan        nan        nan 0.69696429\n",
      " 0.69373277 0.69373277 0.69373277 0.69332707 0.69389724 0.69914526\n",
      " 0.70616802        nan 0.69502663        nan        nan        nan\n",
      "        nan        nan        nan 0.69502663 0.71673768 0.71673768\n",
      " 0.71673768 0.71702694 0.71689641 0.71427684 0.71330148        nan\n",
      " 0.69615393        nan        nan        nan        nan        nan\n",
      "        nan 0.69615393 0.70621136 0.70621136 0.70621136 0.70689693\n",
      " 0.70605263 0.70908417 0.71289578        nan 0.69471178        nan\n",
      "        nan        nan        nan        nan        nan 0.69471178\n",
      " 0.71699666 0.71699666 0.71699666 0.71734023 0.71699666 0.71473214\n",
      " 0.71247337        nan 0.69663012        nan        nan        nan\n",
      "        nan        nan        nan 0.69663012 0.71504177 0.71504177\n",
      " 0.71504177 0.71423402 0.71504177 0.71706245 0.71865497        nan\n",
      " 0.69035871        nan        nan        nan        nan        nan\n",
      "        nan 0.69035871 0.71566207 0.71566207 0.71566207 0.71452799\n",
      " 0.71534461 0.71502402 0.716132          nan 0.6977381         nan\n",
      "        nan        nan        nan        nan        nan 0.6977381\n",
      " 0.72260391 0.72260391 0.72260391 0.72210526 0.72243682 0.72304929\n",
      " 0.72381475        nan 0.69172619        nan        nan        nan\n",
      "        nan        nan        nan 0.69172619 0.71628603 0.71628603\n",
      " 0.71628603 0.71579522 0.71627767 0.71596282 0.71645572        nan\n",
      " 0.69930242        nan        nan        nan        nan        nan\n",
      "        nan 0.69930242 0.72474676 0.72474676 0.72474676 0.72490288\n",
      " 0.72474676 0.72358657 0.72340539        nan 0.68691729        nan\n",
      "        nan        nan        nan        nan        nan 0.68691729\n",
      " 0.71858918 0.71858918 0.71858918 0.71891239 0.71874791 0.71856673\n",
      " 0.71822682        nan 0.69932279        nan        nan        nan\n",
      "        nan        nan        nan 0.69932279 0.7217288  0.7217288\n",
      " 0.7217288  0.72123016 0.7217288  0.72204052 0.72136905        nan\n",
      " 0.68619048        nan        nan        nan        nan        nan\n",
      "        nan 0.68619048 0.69702485 0.69702485 0.69702485 0.69686612\n",
      " 0.69702485 0.69686612 0.6970306         nan 0.6945567         nan\n",
      "        nan        nan        nan        nan        nan 0.6945567\n",
      " 0.70560829 0.70560829 0.70560829 0.70576702 0.70560829 0.70560829\n",
      " 0.70544956        nan 0.66552579        nan        nan        nan\n",
      "        nan        nan        nan 0.66552579 0.64487573 0.64487573\n",
      " 0.64487573 0.64487573 0.64487573 0.64487573 0.64487573        nan\n",
      " 0.66686665        nan        nan        nan        nan        nan\n",
      "        nan 0.66686665 0.65890299 0.65890299 0.65890299 0.65890299\n",
      " 0.65890299 0.65890299 0.65890299        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "2700 fits failed out of a total of 8100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1116 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of ExtraTreesClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1584 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of ExtraTreesClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.74593411 0.74766761 0.74637688]\n",
      "  warnings.warn(\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "1080 fits failed out of a total of 3240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "454 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "626 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67559524 0.66373564 0.66810176 0.63642883 0.64446429 0.70736999\n",
      " 0.71676222 0.62801378 0.66679355 0.63594951 0.62181025 0.63901368\n",
      " 0.68802292 0.63733318 0.68802292 0.63733318 0.66934759 0.63877036\n",
      " 0.61554433 0.66157216 0.67950632 0.62944001 0.61470969 0.6230378\n",
      " 0.62711257 0.62556339 0.63466505 0.63685986 0.63561508 0.64793442\n",
      " 0.64532555 0.56815241 0.64532555 0.56815241 0.63763027 0.57094638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64614348 0.60064954 0.65066233 0.59350459 0.62431182 0.66972483\n",
      " 0.69725956 0.58483057 0.6548303  0.60643797 0.62773783 0.59013367\n",
      " 0.70064197 0.62528378 0.70064197 0.62528378 0.66789813 0.65195593\n",
      " 0.63247415 0.63750261 0.6286534  0.61202538 0.6503715  0.58645703\n",
      " 0.63049734 0.66604193 0.67091844 0.63440842 0.64536158 0.60945985\n",
      " 0.63550099 0.5852595  0.63550099 0.5852595  0.64197838 0.59734701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67559524 0.62432461 0.66810176 0.64928598 0.64446429 0.69840173\n",
      " 0.71676222 0.65368865 0.66679355 0.6080754  0.62181025 0.64197421\n",
      " 0.68802292 0.63733318 0.68802292 0.63733318 0.66934759 0.63877036\n",
      " 0.61554433 0.70815058 0.67950632 0.63343646 0.61470969 0.62222483\n",
      " 0.62711257 0.62556339 0.63466505 0.63685986 0.63561508 0.64793442\n",
      " 0.64532555 0.56815241 0.64532555 0.56815241 0.63763027 0.57094638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67559524 0.66373564 0.66810176 0.63642883 0.64446429 0.70736999\n",
      " 0.71676222 0.62801378 0.66679355 0.63594951 0.62181025 0.63901368\n",
      " 0.68802292 0.63733318 0.68802292 0.63733318 0.66934759 0.63877036\n",
      " 0.61554433 0.66292137 0.67950632 0.62944001 0.61470969 0.6230378\n",
      " 0.62711257 0.62556339 0.63466505 0.63685986 0.63561508 0.64793442\n",
      " 0.64532555 0.56815241 0.64532555 0.56815241 0.63763027 0.57094638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.68105603 0.64443426 0.68626279 0.61451441 0.70011226 0.65489166\n",
      " 0.66259425 0.66259973 0.6757806  0.67438231 0.72827068 0.63820254\n",
      " 0.72052553 0.58527491 0.72052553 0.58527491 0.65203504 0.56703843\n",
      " 0.62346543 0.61932879 0.61439093 0.72694914 0.63742847 0.61287542\n",
      " 0.63852548 0.65730994 0.62609519 0.66941808 0.65290779 0.63644476\n",
      " 0.65517152 0.61442356 0.65517152 0.61442356 0.65013027 0.56097979\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71418677 0.61246658 0.65555112 0.63501906 0.68396277 0.57499269\n",
      " 0.64635704 0.62849415 0.68402491 0.59506892 0.69585605 0.57577224\n",
      " 0.71543233 0.60777647 0.71543233 0.60777647 0.66057722 0.62575084\n",
      " 0.6336722  0.65899044 0.64324614 0.64364792 0.63745901 0.60753472\n",
      " 0.61935725 0.65327433 0.63800804 0.66932174 0.63118917 0.6246426\n",
      " 0.62497389 0.59804381 0.62497389 0.59804381 0.61944758 0.57103122\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.68018301 0.65310229 0.68626279 0.6222713  0.70011226 0.64671079\n",
      " 0.66259425 0.66210631 0.6757806  0.68282007 0.72827068 0.64272635\n",
      " 0.72052553 0.58527491 0.72052553 0.58527491 0.65203504 0.56703843\n",
      " 0.63170792 0.68067304 0.62576963 0.69875444 0.65389724 0.63254699\n",
      " 0.6523243  0.66532581 0.65391917 0.67161315 0.65636174 0.63644476\n",
      " 0.65517152 0.61442356 0.65517152 0.61442356 0.65013027 0.56097979\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.68105603 0.64443426 0.68626279 0.61451441 0.70011226 0.65489166\n",
      " 0.66259425 0.66259973 0.6757806  0.67438231 0.72827068 0.63820254\n",
      " 0.72052553 0.58527491 0.72052553 0.58527491 0.65203504 0.56703843\n",
      " 0.62346543 0.62953451 0.61439093 0.72694914 0.63742847 0.61287542\n",
      " 0.63786759 0.65730994 0.62609519 0.66941808 0.65290779 0.63644476\n",
      " 0.65517152 0.61442356 0.65517152 0.61442356 0.65013027 0.56097979\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.68105603 0.64443426 0.68626279 0.61451441 0.70011226 0.65489166\n",
      " 0.66259425 0.66259973 0.6757806  0.67438231 0.72827068 0.63820254\n",
      " 0.72052553 0.58527491 0.72052553 0.58527491 0.65203504 0.56703843\n",
      " 0.62346543 0.61932879 0.61439093 0.72694914 0.63742847 0.61287542\n",
      " 0.63852548 0.65730994 0.62609519 0.66941808 0.65290779 0.63644476\n",
      " 0.65517152 0.61442356 0.65517152 0.61442356 0.65013027 0.56097979\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71418677 0.61246658 0.65555112 0.63501906 0.68396277 0.57499269\n",
      " 0.64635704 0.62849415 0.68402491 0.59506892 0.69585605 0.57577224\n",
      " 0.71543233 0.60777647 0.71543233 0.60777647 0.66057722 0.62575084\n",
      " 0.6336722  0.65899044 0.64324614 0.64364792 0.63745901 0.60753472\n",
      " 0.61935725 0.65327433 0.63800804 0.66932174 0.63118917 0.6246426\n",
      " 0.62497389 0.59804381 0.62497389 0.59804381 0.61944758 0.57103122\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.68018301 0.65310229 0.68626279 0.6222713  0.70011226 0.64671079\n",
      " 0.66259425 0.66210631 0.6757806  0.68282007 0.72827068 0.64272635\n",
      " 0.72052553 0.58527491 0.72052553 0.58527491 0.65203504 0.56703843\n",
      " 0.63170792 0.68067304 0.62576963 0.69875444 0.65389724 0.63254699\n",
      " 0.6523243  0.66532581 0.65391917 0.67161315 0.65636174 0.63644476\n",
      " 0.65517152 0.61442356 0.65517152 0.61442356 0.65013027 0.56097979\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.68105603 0.64443426 0.68626279 0.61451441 0.70011226 0.65489166\n",
      " 0.66259425 0.66259973 0.6757806  0.67438231 0.72827068 0.63820254\n",
      " 0.72052553 0.58527491 0.72052553 0.58527491 0.65203504 0.56703843\n",
      " 0.62346543 0.62953451 0.61439093 0.72694914 0.63742847 0.61287542\n",
      " 0.63786759 0.65730994 0.62609519 0.66941808 0.65290779 0.63644476\n",
      " 0.65517152 0.61442356 0.65517152 0.61442356 0.65013027 0.56097979]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    best_model, train_accuracy, test_accuracy, best_params, best_score, train_f1, test_f1 = train_evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, model_name)\n",
    "    best_models[model_name] = best_model\n",
    "    result = {}\n",
    "    result[model_name] = [train_accuracy, test_accuracy, train_f1, test_f1, best_score, best_params]\n",
    "    results = pd.DataFrame(result).T\n",
    "    results.columns = ['Train Accuracy', 'Test Accuracy', 'Train F1', 'Test F1', 'Best Score', 'Best Params']\n",
    "    path = model_name + \".json\"\n",
    "    results.T[model_name].to_json(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train F1</th>\n",
       "      <th>Test F1</th>\n",
       "      <th>Best Score</th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.873874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.675551</td>\n",
       "      <td>{'alpha': 0, 'gamma': 0.001, 'lambda': 0, 'lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.891156</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.469252</td>\n",
       "      <td>{'C': 1000, 'penalty': 'l2', 'solver': 'newton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.671202</td>\n",
       "      <td>0.693694</td>\n",
       "      <td>0.521452</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.472757</td>\n",
       "      <td>{'alpha': 0.001, 'fit_intercept': False, 'posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.506845</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 20, 'max_fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.538327</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'learning_rate': 1.5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Passive Aggressive</th>\n",
       "      <td>0.77551</td>\n",
       "      <td>0.774775</td>\n",
       "      <td>0.497462</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.360952</td>\n",
       "      <td>{'C': 0.1, 'loss': 'hinge'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Classification</th>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.828829</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.534743</td>\n",
       "      <td>{'C': 100, 'decision_function_shape': 'ovo', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Trees</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.493714</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 20, 'max...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828829</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.553727</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': None, 'm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Train Accuracy Test Accuracy  Train F1  \\\n",
       "XGBClassifier                            1.0      0.873874       1.0   \n",
       "Logistic Regression                 0.891156      0.783784  0.752577   \n",
       "Ridge                               0.671202      0.693694  0.521452   \n",
       "Extra Trees                              1.0      0.810811       1.0   \n",
       "AdaBoost                                 1.0      0.810811       1.0   \n",
       "Passive Aggressive                   0.77551      0.774775  0.497462   \n",
       "Support Vector Classification       0.968254      0.828829      0.93   \n",
       "Decision Trees                           1.0      0.756757       1.0   \n",
       "Random Forest                            1.0      0.828829       1.0   \n",
       "\n",
       "                                Test F1 Best Score  \\\n",
       "XGBClassifier                  0.681818   0.675551   \n",
       "Logistic Regression            0.538462   0.469252   \n",
       "Ridge                          0.514286   0.472757   \n",
       "Extra Trees                    0.432432   0.506845   \n",
       "AdaBoost                       0.487805   0.538327   \n",
       "Passive Aggressive             0.468085   0.360952   \n",
       "Support Vector Classification  0.612245   0.534743   \n",
       "Decision Trees                 0.470588   0.493714   \n",
       "Random Forest                  0.457143   0.553727   \n",
       "\n",
       "                                                                     Best Params  \n",
       "XGBClassifier                  {'alpha': 0, 'gamma': 0.001, 'lambda': 0, 'lea...  \n",
       "Logistic Regression            {'C': 1000, 'penalty': 'l2', 'solver': 'newton...  \n",
       "Ridge                          {'alpha': 0.001, 'fit_intercept': False, 'posi...  \n",
       "Extra Trees                    {'criterion': 'gini', 'max_depth': 20, 'max_fe...  \n",
       "AdaBoost                       {'algorithm': 'SAMME.R', 'learning_rate': 1.5,...  \n",
       "Passive Aggressive                                   {'C': 0.1, 'loss': 'hinge'}  \n",
       "Support Vector Classification  {'C': 100, 'decision_function_shape': 'ovo', '...  \n",
       "Decision Trees                 {'criterion': 'entropy', 'max_depth': 20, 'max...  \n",
       "Random Forest                  {'criterion': 'entropy', 'max_depth': None, 'm...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results = pd.DataFrame(result).T\n",
    "# results.columns = ['Train Accuracy', 'Test Accuracy', 'Train F1', 'Test F1', 'Best Score', 'Best Params']\n",
    "# results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in models.keys():\n",
    "    # print(results[i])\n",
    "    path = i + \".json\"\n",
    "    results.T[i].to_json(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0,\n",
       " 'gamma': 0.001,\n",
       " 'lambda': 0,\n",
       " 'learning_rate': 1.0,\n",
       " 'n_estimators': 150}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['Best Params'].iloc[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
